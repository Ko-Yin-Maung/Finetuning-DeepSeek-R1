{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABD9ONh0V1xV",
        "outputId": "d047c09c-5255-42c4-83e9-39369c10ab90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zvk0UxqgVp_n",
        "outputId": "af5d770e-6fec-44d4-d5bd-31afe0bbbd5c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.3.2-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Collecting trl\n",
            "  Downloading trl-0.15.2-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.14.0)\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.45.3-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.13)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: accelerate>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from trl) (1.3.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from trl) (13.9.4)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.3.2-py3-none-any.whl (485 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m485.4/485.4 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m108.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m78.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m53.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m67.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trl-0.15.2-py3-none-any.whl (318 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitsandbytes-0.45.3-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, dill, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, nvidia-cusolver-cu12, datasets, bitsandbytes, trl\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed bitsandbytes-0.45.3 datasets-3.3.2 dill-0.3.8 multiprocess-0.70.16 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 trl-0.15.2 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers datasets torch trl peft bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "id": "32t39hlZVqAW",
        "outputId": "5d562657-f410-4fcd-f1f4-edc6561ac3e6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-15-590a351adf3a>:70: FutureWarning: `tokenizer` is deprecated and removed starting from version 0.16.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = SFTTrainer(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/trainer.py:3441: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  torch.load(os.path.join(checkpoint, OPTIMIZER_NAME), map_location=map_location)\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/trainer.py:3105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint_rng_state = torch.load(rng_file)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='29133' max='33240' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [29133/33240 21:07 < 1:00:38, 1.13 it/s, Epoch 105.15/120]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>27800</td>\n",
              "      <td>0.034700</td>\n",
              "      <td>0.981139</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28000</td>\n",
              "      <td>0.033000</td>\n",
              "      <td>0.974706</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28200</td>\n",
              "      <td>0.035200</td>\n",
              "      <td>0.974130</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28400</td>\n",
              "      <td>0.034400</td>\n",
              "      <td>0.974533</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28600</td>\n",
              "      <td>0.032400</td>\n",
              "      <td>0.974873</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28800</td>\n",
              "      <td>0.039400</td>\n",
              "      <td>0.975019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29000</td>\n",
              "      <td>0.035000</td>\n",
              "      <td>0.981379</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    TrainingArguments,\n",
        ")\n",
        "from trl import SFTTrainer\n",
        "import torch\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "dataset = load_dataset(\"csv\", data_files=\"/content/drive/MyDrive/R1-1.5B/Microbiology.csv\")\n",
        "\n",
        "def format_dataset(examples):\n",
        "    texts = []\n",
        "    for instruction, response in zip(examples[\"Instruction\"], examples[\"Output\"]):\n",
        "        formatted_text = f\"### Instruction:\\n{instruction}\\n\\n### Response:\\n{response}\"\n",
        "        texts.append(formatted_text)\n",
        "    return {\"text\": texts}\n",
        "\n",
        "dataset = dataset.map(format_dataset, batched=True, remove_columns=[\"Instruction\", \"Output\"])\n",
        "\n",
        "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")\n",
        "# model = model.to('cuda')\n",
        "\n",
        "## Configure LoRA\n",
        "peft_config = LoraConfig(\n",
        "    r=16,                   # Rank of the low-rank matrices\n",
        "    lora_alpha=32,          # Scaling factor\n",
        "    lora_dropout=0.1,       # Dropout for LoRA layers\n",
        "    bias=\"none\",            # No bias for LoRA\n",
        "    task_type=\"CAUSAL_LM\",  # Task type\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]  # Target modules for LoRA\n",
        ")\n",
        "model = get_peft_model(model, peft_config)\n",
        "\n",
        "## Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/drive/MyDrive/R1-1.5B/results\",  # Directory to save results\n",
        "    num_train_epochs=120,           # Number of training epochs\n",
        "    per_device_train_batch_size=2,  # Batch size per device\n",
        "    per_device_eval_batch_size=2,   # Evaluation batch size\n",
        "    gradient_accumulation_steps=2,  # Gradient accumulation steps\n",
        "    gradient_checkpointing=False,   # Disable gradient checkpointing for debugging\n",
        "    optim=\"adamw_torch\",            # Optimizer\n",
        "    learning_rate=1e-4,             # Learning rate\n",
        "    warmup_ratio=0.1,               # Warmup ratio\n",
        "    fp16=True,                      # Use mixed precision (FP16)\n",
        "    logging_steps=200,              # Log every 10 steps\n",
        "    save_strategy=\"steps\",          # Save model at specific steps\n",
        "    save_steps=200,                 # Save every 200 steps\n",
        "    save_total_limit=2,             # Save steps\n",
        "    evaluation_strategy=\"steps\",    # Evaluate at specific steps\n",
        "    eval_steps=200,                 # Evaluate every 200 steps\n",
        "    eval_accumulation_steps=1,      # Accumulate evaluation steps\n",
        "    load_best_model_at_end=True,    # Load the best model at the end\n",
        "    metric_for_best_model=\"eval_loss\",  # Metric for best model\n",
        "    greater_is_better=False,        # Lower eval_loss is better\n",
        "    remove_unused_columns=True,     # Remove unused columns\n",
        "    report_to=\"none\",               # Disable external logging\n",
        ")\n",
        "\n",
        "## Split Train and Test\n",
        "split_dataset = dataset[\"train\"].train_test_split(test_size=150, seed=42)\n",
        "train_dataset = split_dataset[\"train\"]\n",
        "eval_dataset = split_dataset[\"test\"]\n",
        "\n",
        "## Initialize the trainer\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    args = training_args,\n",
        "    # train_dataset=dataset[\"train\"],\n",
        "    # eval_dataset=dataset[\"train\"].select(range(120)),  # Small evaluation set\n",
        "    train_dataset = train_dataset,\n",
        "    eval_dataset = eval_dataset,\n",
        "    tokenizer = tokenizer,\n",
        ")\n",
        "\n",
        "## Train the model\n",
        "# trainer.train()\n",
        "checkpoint = '/content/drive/MyDrive/R1-1.5B/results/checkpoint-27700'\n",
        "trainer.train(resume_from_checkpoint=checkpoint)\n",
        "\n",
        "## Save the model\n",
        "trained_model = \"/content/drive/MyDrive/R1-1.5B/fine-tuned-deepseek-r1-1.5b\"\n",
        "model.save_pretrained(trained_model)\n",
        "tokenizer.save_pretrained(trained_model)\n",
        "# # trainer.save_model(trained_model)\n",
        "# trainer.model.save_pretrained(trained_model)\n",
        "# trainer.tokenizer.save_pretrained(trained_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HI1DpDsTVqAe",
        "outputId": "5d064c28-03fd-4c05-f32f-85fd642529c0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "á€†á€±á€¸á€›á€¯á€¶á€á€½á€„á€ºá€¸ á€€á€°á€¸á€…á€€á€ºá€›á€±á€¬á€‚á€« á€™á€»á€¬á€¸á€€á€¼á€±á€¬á€„á€·á€º á€…á€®á€¸á€•á€½á€¬á€¸á€›á€±á€¸á€¡á€•á€±á€«á€º á€á€€á€ºá€›á€±á€¬á€€á€ºá€™á€¾á€¯á€™á€»á€¬á€¸á€€á€­á€¯ á€–á€±á€¬á€ºá€•á€¼á€•á€«á€á€Šá€º á‹\n",
            "\n",
            "### á€…á€¬á€¸á€á€¯á€¶á€¸á€á€¼á€„á€ºá€¸\n",
            "- á€€á€¼á€½á€€á€ºá€á€¬á€¸á€™á€»á€¬á€¸á€á€­á€¯á€· á€™á€¾á€­á€¯á€á€»á€­á€”á€ºá€€á€¼á€½á€€á€ºá€€á€­á€¯ á€…á€½á€™á€ºá€¸á€¡á€„á€ºá€¡á€™á€»á€¬á€¸á€…á€¯ á€›á€¾á€­á€á€Šá€º á‹\n",
            "- á€€á€¼á€½á€€á€ºá€á€¬á€¸á€™á€»á€¬á€¸á€á€­á€¯á€· á€›á€±á€¬á€€á€ºá€›á€¾á€­á€á€¼á€„á€ºá€¸ á€™á€¾á€á€…á€ºá€†á€„á€·á€º á€€á€°á€¸á€…á€€á€ºá€”á€­á€¯á€„á€ºá€á€Šá€º á‹\n",
            "- á€€á€¼á€½á€€á€ºá€á€¬á€¸á€™á€»á€¬á€¸á€á€­á€¯á€· á€•á€»á€€á€ºá€…á€®á€¸á€™á€¾á€¯ á€™á€»á€­á€¯á€¸á€…á€­á€á€ºá€™á€»á€¬á€¸ á€›á€¾á€­á€á€Šá€º á‹\n",
            "- á€€á€¼á€½á€€á€ºá€á€¬á€¸á€™á€»á€¬á€¸á€á€­á€¯á€· á€›á€±á€¬á€€á€ºá€›á€¾á€­á€á€¼á€„á€ºá€¸ á€™á€¾á€á€…á€ºá€†á€„á€·á€º á€€á€°á€¸á€…á€€á€ºá€”á€­á€¯á€„á€ºá€á€Šá€º á‹\n",
            "- á€€á€¼á€½á€€á€ºá€á€¬á€¸á€™á€»á€¬á€¸á€á€­á€¯á€· á€›á€±á€¬á€€á€ºá€›á€¾á€­á€á€¼á€„á€ºá€¸ á€™á€¾á€á€…á€ºá€†á€„á€·á€º á€€á€°á€¸á€…á€€á€ºá€”á€­á€¯á€„á€ºá€á€Šá€º á‹\n",
            "- á€€á€¼á€½á€€á€ºá€á€¬á€¸á€™á€»á€¬á€¸á€á€­á€¯á€·\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "model_path = \"/content/drive/MyDrive/R1-1.5B/fine-tuned-deepseek-r1-1.5b\"\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_path,\n",
        "    torch_dtype=torch.float16,\n",
        "    # device_map=\"auto\"\n",
        ")\n",
        "model = model.to('cuda')\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "\n",
        "def generate_text(prompt, max_new_tokens=512):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to('cuda')\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=True,\n",
        "            temperature=0.5,\n",
        "            top_k=50,\n",
        "            top_p=0.9,\n",
        "            use_cache=True\n",
        "        )\n",
        "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "prompt = \"á€†á€±á€¸á€›á€¯á€¶á€á€½á€„á€ºá€¸ á€€á€°á€¸á€…á€€á€ºá€›á€±á€¬á€‚á€« á€™á€»á€¬á€¸á€€á€¼á€±á€¬á€„á€·á€º á€…á€®á€¸á€•á€½á€¬á€¸á€›á€±á€¸á€¡á€•á€±á€«á€º á€á€€á€ºá€›á€±á€¬á€€á€ºá€™á€¾á€¯á€™á€»á€¬á€¸á€€á€­á€¯ á€–á€±á€¬á€ºá€•á€¼á€•á€«\"\n",
        "output = generate_text(prompt)\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v4zCYeq_ALGW",
        "outputId": "853b70ac-2bb2-4995-d35b-901b783650c6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selective agar á€á€Šá€º á€™á€¾á€á€ºá€á€­á€™á€®erobita á€˜á€€á€ºá€á€®á€¸á€›á€®á€¸á€šá€¬á€¸ á€¡á€™á€»á€­á€¯á€¸á€¡á€…á€¬á€¸ á á€á€¯á€¶á€·á€›á€¾á€­ á€•á€¯á€¶á€á€á€¹á€á€¬á€”á€º á€€á€°á€¸á€…á€€á€ºá€•á€»á€¶á€·á€”á€¾á€¶á€·á€á€±á€¬ á€¡á€…á€¬á€¸á€¡á€…á€¬ á€™á€»á€¬á€¸á€€á€­á€¯ á€™á€»á€¾á€á€±á€á€¯á€¶á€¸á€…á€½á€²á€á€¼á€„á€ºá€¸ á€–á€¼á€…á€ºá€á€Šá€º á‹ áá€„á€ºá€¸á€á€½á€„á€º Gram-positive á€˜á€€á€ºá€á€­á€™á€®erobita á€˜á€€á€ºá€á€­á€™á€®erobita á€¡á€á€½á€€á€º á€™á€½á€±á€¸á€™á€¼á€°á€á€Šá€º á‹ Gram-positive á€˜á€€á€ºá€á€­á€™á€®erobita á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€›á€±á€¸á€œá€»á€¾á€„á€º selective agar á€™á€»á€¬á€¸ á€–á€¼á€…á€ºá€•á€±á€«á€ºá€…á€±á€•á€¼á€®á€¸ Gram-negative á€˜á€€á€ºá€á€­á€™á€®erobita á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€›á€±á€¸ á€‘á€¯á€á€ºá€œá€½á€¾á€€á€ºá€á€Šá€º á‹ á€¡á€–á€»á€¬á€¸á€†á€¯á€¶á€¸ á€á€½á€„á€º á€•á€¼á€„á€ºá€¸á€‘á€”á€ºá€á€±á€¬ á€˜á€€á€ºá€á€­á€™á€®erobita á€™á€»á€¬á€¸á€€á€­á€¯ á€á€½á€±ï¿½\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "model_path = \"/content/drive/MyDrive/R1-1.5B/fine-tuned-deepseek-r1-1.5b\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16)\n",
        "model = model.to('cuda')\n",
        "\n",
        "instruction = \"Selective agar á€†á€­á€¯á€á€¬ á€˜á€¬á€œá€² á€›á€¾á€„á€ºá€¸á€•á€¼á€•á€«\"\n",
        "input_text = f\"### Instruction:\\n{instruction}\\n\\n### Response:\"\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        inputs[\"input_ids\"],\n",
        "        attention_mask=inputs[\"attention_mask\"],\n",
        "        max_length=512,\n",
        "        temperature=0.7,\n",
        "        top_k=50,\n",
        "        top_p=0.95,\n",
        "        do_sample = True,\n",
        "        use_cache=True,\n",
        "        num_return_sequences=1\n",
        "    )\n",
        "\n",
        "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "response = response.split(\"### Response:\")[-1].strip()\n",
        "print(response)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
